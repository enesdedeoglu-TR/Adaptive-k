{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-recall",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T14:44:08.401971Z",
     "start_time": "2022-03-26T14:44:07.511161Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-harvey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T14:44:08.450015Z",
     "start_time": "2022-03-26T14:44:08.435001Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 15, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(15, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), 2, stride=2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-offering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T16:05:05.645594Z",
     "start_time": "2022-03-26T16:05:05.633584Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We manually add directed or random label noise to randomly \n",
    "selected samples from the training set.\n",
    "'''\n",
    "def noisy_loader(params):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                        datasets.MNIST('../data', train=True, download=True,\n",
    "                                       transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                     transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                       batch_size=params['subsampled_number'], shuffle=True)    \n",
    "    for index, (data, target) in enumerate(train_loader):\n",
    "        flag = np.random.binomial(1, params['epsilon'], size=(len(target), 1))\n",
    "        target_noisy = copy.deepcopy(target.numpy())\n",
    "        if params['clean_data'] is False:\n",
    "            for index, val in enumerate(flag):\n",
    "                if val[0] == 1 and params['noise_type'] == 'directed':\n",
    "                    target_noisy[index] = (target[index] + params['shift']) % 10\n",
    "                if val[0] == 1 and params['noise_type'] == 'random':\n",
    "                    out = np.random.randint(0, 10)\n",
    "                    while out == target_noisy[index]:\n",
    "                        out = np.random.randint(0, 10)\n",
    "                    target_noisy[index] = out\n",
    "            break\n",
    "        elif params['clean_data'] is True:\n",
    "            print('Do nothing')\n",
    "    target_noisy = torch.from_numpy(target_noisy)\n",
    "    train_noisy = torch.utils.data.TensorDataset(data, target_noisy)    \n",
    "    train_loader_noisy = torch.utils.data.DataLoader(train_noisy, \n",
    "                                                     batch_size=params['minibatch'], \n",
    "                                                     drop_last=True,\n",
    "                                                     shuffle=True)\n",
    "    return train_loader_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-sweet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T16:15:03.950358Z",
     "start_time": "2022-03-26T16:15:03.924335Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader_noisy, epoch, run, epsilon, params):\n",
    "    model.train()\n",
    "    # Beta parameters required to calculate moving average\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999     \n",
    "    # Number of epochs to be trained with SGD(Vanilla)    \n",
    "    vanilla_limit = 30    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader_noisy):\n",
    "        data, target = Variable(data.cuda(), requires_grad=True), Variable(target.cuda())        \n",
    "        if params['optimizer_type'] == 'sgd':            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "        elif params['optimizer_type'] == 'adaptive-k':\n",
    "            if epoch < (vanilla_limit+1): # SGD(Vanilla)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "            else:\n",
    "                output = model(data) \n",
    "                temp_loss = F.nll_loss(output, target.cuda(), reduction='none')\n",
    "                temp = temp_loss.cpu().detach().numpy()\n",
    "                mini_batch_mean = np.mean(temp)\n",
    "                #We initialize the m and v parameters from zero. \n",
    "                #Before each epoch, we get the last m and v parameters from the previous epoch. \n",
    "                if batch_idx == 0:\n",
    "                    m = results['m'][(epoch-2),run]  \n",
    "                    v = results['v'][(epoch-2),run]   \n",
    "                m = beta1*m + (1-beta1)*mini_batch_mean\n",
    "                v = beta2*v + (1-beta2)*mini_batch_mean*mini_batch_mean\n",
    "                moving_avg = m / (np.sqrt(v) + 0.00000001)                     \n",
    "                index1 = np.where(temp <= moving_avg )[0] # we choose from mini-batch with threshold value(moving average).                   \n",
    "                if len(index1) != 0:    \n",
    "                    data1 = data[index1[:],:,:,:].view(len(index1), 1, 28, 28)\n",
    "                    target1 = target[index1[:]]\n",
    "                    data1, target1 = Variable(data1.cuda()), Variable(target1.cuda())\n",
    "                    output1 = model(data1)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.nll_loss(output1, target1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 250 == 249:\n",
    "            print('Train Run: {} Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, {}'.format(\n",
    "                run + 1 , epoch, batch_idx * len(data), len(train_loader_noisy.dataset),\n",
    "                100. * batch_idx / len(train_loader_noisy), loss.item(), params['optimizer_type']))                \n",
    "    if params['optimizer_type'] == 'adaptive-k' and epoch > vanilla_limit:        \n",
    "        results['moving_avg'][(epoch-1),run] = moving_avg \n",
    "        results['m'][(epoch-1),run] = m \n",
    "        results['v'][(epoch-1),run] = v \n",
    "    return loss.item()\n",
    "\n",
    "def test(test_loader, run):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).float().cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set {} Run: {} : Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            params['optimizer_type'], run + 1, test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "def select_optimizer(optimizer_name, params):\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['lr'])\n",
    "    elif optimizer_name == 'sgdmomentum':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=params['momentum'])\n",
    "    return optimizer\n",
    "\n",
    "def one_run(train_loader_noisy, model, optimizer, params, results):\n",
    "    num_epochs = params['num_epochs']\n",
    "    count = params['current_run']\n",
    "    time_start = time.time()\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        if params['decayschedule'] != 0:\n",
    "            scheduler.step()\n",
    "        results['train_loss'][epoch - 1, count] = train(train_loader_noisy, epoch, params['current_run'], params['epsilon'], params)\n",
    "        results['test_loss'][epoch - 1, count], results['test_acc'][epoch - 1, count] = test(test_loader, params['current_run'])\n",
    "        results['time_spent'][epoch - 1, count] = time.time() - time_start\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-parts",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T16:15:04.422740Z",
     "start_time": "2022-03-26T16:15:04.412731Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    params = {}\n",
    "    params['lr'] = 0.05                        # Learning rate\n",
    "    params['momentum'] = 0.0                   # Momentum parameter\n",
    "    params['minibatch'] = 10                   # Number of loss evaluations per batch\n",
    "    params['subsampled_number'] = 5000         # Size of dataset, parameter used in noisy_loader\n",
    "    params['decayschedule'] = 30               # Decay Schedule\n",
    "    params['noise_type'] = 'directed'          # Noise type\n",
    "                                               #    To use directed noise model of corruption, set 'directed'\n",
    "                                               #    To use random noise model of corruption, set 'random'\n",
    "    params['learningratedecay'] = 0.2          # Learning Rate Decay\n",
    "    params['eps']= 1e-08                       # Corruption parameter\n",
    "    params['num_epochs'] = 80                  # Number of epochs\n",
    "    params['optimizer_type'] = 'adaptive-k'    # Optimizer Type:\n",
    "                                               #    To run standard stochastic gradient descent, set 'sgd'\n",
    "                                               #    To run standard Adaptive-k, set 'adaptive-k'\n",
    "    params['runs'] = 1                         # Number of runs\n",
    "    params['epsilon'] = 0.4                    # Fraction of corrupted data \n",
    "        \n",
    "    params['clean_data'] = False               # Flag to only consider clean data\n",
    "    params['shift'] = 2                        # Shift parameter in directed noise model\n",
    "\n",
    "    results = {}\n",
    "    results['train_loss'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['test_loss'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['test_acc'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['time_spent'] = np.zeros((params['num_epochs'], params['runs'])) \n",
    "    \n",
    "    results['moving_avg'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    \n",
    "    results['m'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "    results['v'] = np.zeros((params['num_epochs'], params['runs']))\n",
    "   \n",
    "    transform_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    trainset = datasets.MNIST('../data', train=True, download=True, transform=transform_train)\n",
    "    testset = datasets.MNIST('../data', train=False, transform = transform_test)\n",
    "    test_loader = torch.utils.data.DataLoader(testset,batch_size = 1000, shuffle=False)\n",
    "    return params, trainset, test_loader, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-bruce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T16:18:42.079873Z",
     "start_time": "2022-03-26T16:15:04.901055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "time_start = time.time()\n",
    "params, trainset, test_loader, results = init_params()\n",
    "for run in range(params['runs']):\n",
    "    train_loader_noisy = noisy_loader(params)\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    optimizer = select_optimizer('sgd', params)\n",
    "    if params['decayschedule'] != 0:\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=params['decayschedule'], gamma=params['learningratedecay'])\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))        \n",
    "    params['current_run'] = run\n",
    "    results = one_run(train_loader_noisy, model, optimizer, params, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-baseline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T16:18:42.739664Z",
     "start_time": "2022-03-26T16:18:42.732658Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average of the test accuracies of the tests in the last epoch: \",np.mean(results[\"test_acc\"][-1]))\n",
    "max_avg = 0\n",
    "for i in range(params['runs']):\n",
    "    print(\"The maximum test accuracy of the {}th test : {}\".format((i+1), np.max(results[\"test_acc\"][:,i]) ))\n",
    "    max_avg = max_avg + np.max(results[\"test_acc\"][:,i])\n",
    "print(\"Average of maximum test accuracies: \", max_avg/params['runs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-emergency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-26T15:51:20.233709Z",
     "start_time": "2022-03-26T15:51:20.184672Z"
    }
   },
   "outputs": [],
   "source": [
    "if params['optimizer_type'] == 'sgd':\n",
    "    file_path = \"./results/%s_%s/lr_%.4f_momentum_%.4f_eps_%.4f_ds_%d_n_epochs_%d_runs_%d_minibatch_%d\"\\\n",
    "                % (params['optimizer_type'], params['noise_type'], params['lr'], \n",
    "                   params['momentum'], params['epsilon'], params['decayschedule'], \n",
    "                   params['num_epochs'], params['runs'], params['k'])\n",
    "elif params['optimizer_type'] == 'adaptive-k':\n",
    "    file_path = \"./results/%s_%s/lr_%.4f_momentum_%.4f_eps_%.4f_ds_%d_n_epochs_%d_runs_%d_minibatch_%d_frac_%.2f\"\\\n",
    "                % (params['optimizer_type'], params['noise_type'], params['lr'], \n",
    "                   params['momentum'], params['epsilon'], params['decayschedule'], \n",
    "                   params['num_epochs'], params['runs'], params['k'], params['frac'])\n",
    "\n",
    "directory = os.path.dirname(file_path)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "np.savez(file_path, train_loss=results['train_loss'], test_loss=results['test_loss'], test_acc=results['test_acc'], time_spent=results['time_spent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-resident",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
